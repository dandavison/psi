#+title:Variational Bayes and parallel algorithms for fitting mixture models to large genotype data sets
#+author:Dan Davison
#+date:\today

* Introduction
Inference of population structure is often the main motivation for
creating data sets of genetic variation at the level of species and
populations. In addition, information about population structure is
often required in other inference problems. A notable example is
genome-wide association study of common diseases in humans. These
studies seek to identify genomic regions within which genetic
variation is associated with the phenotype of interest. Anticipated
penetrance is typically very low for phenotypes of interest, with the
result that well-powered studies require large sample sizes. In this
situation, subtle correlations between genome-wide ancestry and
phenotype result in spurious association signals in a naive
analysis. This problem can be avoided to a large degree if information
about ancestry (i.e. population structure) is available.

Starting with the work of \citet{Pritchard_et_al_2000} (PSD), whose
software package =structure= has been widely used during the last
decade, several studies have developed Bayesian mixture models for
multilocus genotype data with the objective of characterising
population structure \citep{Pritchard_et_al_2000, Corander_et_al_2003,
Guillot_et_al_2005, Huelsenbeck_Andolfatto_2007,
Leslie_in_prep}. However, in the last five years those analysing large
SNP data sets (10^5-10^6 loci, 10^3-10^4 individuals) have largely
abandoned these methods due to the long computing times required by
the available implementations, turning instead to Principal Components
Analysis (PCA) \citep{Patterson,Price,Vukcevic?}. Here I describe two
new approaches for fitting mixture models to genotype data, which can
result in practical computing times when analysing such large data
sets. The first approach is variational Bayes. VB offers the
possibility of obtaining approximate posterior densities via an
EM-like hill-climbing algorithm. The second approach is parallel
computing, which results in computation times which scale inversely
with the number of processors available. I describe and implement
parallel algorithms for EM, VB and MCMC.
* Overview
:PROPERTIES:
:ID:       36c71055-e7db-4325-8c63-ea62130b873e
:END:
In section \ref{sec:models} I describe the models and introduce
notation. The models studied are the "No Admixture Model" and the
"Admixture Model" of \citet{Pritchard_et_al_2000}, and the "F Model" of
\citet{Falush_et_al_2003}. In section \ref{sec:model-fitting} I
describe new algorithms for fitting the three models. In each case I
start with the EM algorithm, which provides a helpful comparison with
the corresponding VB algorithm. Finally I describe parallel versions
of the same algorithms. Additional derivation of the updates used are
provided in the \ref{sec:appendix}. Section \ref{sec:results} presents
results of using the different methods to fit the models to simulated
and real data. Finally in section \ref{sec:discussion} I discuss
potential applications of these methods. The algorithms are
implemented in the software package =psi= available at [psi
URL]. Table \ref{tbl:notation} contains notation used throughout the
paper.

#+caption: Notation used
#+label: tbl:notation
| Notation             | Meaning                                                                                           |
|----------------------+---------------------------------------------------------------------------------------------------|
| $n$                  | Number of individuals                                                                             |
| $L$                  | Number of marker loci                                                                             |
| $J_l$                | Number of alleles at locus l                                                                      |
| $K$                  | Number of groups in mixture model                                                                 |
| $i$                  | Indexes individuals: i \in {1,\ldots,n}                                                           |
| $l$                  | Indexes loci: l \in {1,\ldots,L}                                                                  |
| $j$                  | Indexes alleles: j \in {1,\ldots,J_l}                                                             |
| $a$                  | Indexes chromosomes: a \in {1,2} = {maternal,paternal}                                            |
| $X_{ila}$            | Identity of allele on chromosome a at locus l in individual i: X_ila \in {1,\ldots,J_l}           |
| $\phi_k$             | Model parameters in mixture model with k groups: \phi_k=(\mu_k, ...?)                             |
| $P_{lk}$             | Allele frequencies at locus l in group k                                                          |
| $\alpha_j^{(0)}$     | Hyperparameter of Dirichlet prior on allele frequencies (same for all l, k)                       |
| $\alpha_{lkj}^{(1)}$ | Hyperparameter of Dirichlet posterior on allele frequencies at locus l in group k                 |
| $~$                  |                                                                                                   |
|                      |                                                                                                   |
| No Admixture Model   |                                                                                                   |
|----------------------+---------------------------------------------------------------------------------------------------|
| Z_i                  | Unknown group label for individual i: Z_i \in {1,\ldots,K}                                        |
| $\pi_k$              | Intensity of group k in the mixture                                                               |
| $~$                  |                                                                                                   |
|                      |                                                                                                   |
| Admixture Model      |                                                                                                   |
|----------------------+---------------------------------------------------------------------------------------------------|
| Z_{ila}              | Unknown group label for allele on chromosome a at locus l in individual i: Z_ila \in {1,\ldots,K} |
| $Q_{ik}$             | Unknown genome-wide proportion of ancestry of individual i from group k                           |
| $\lambda_k^{(0)}$    | Hyperparameter of  Dirichlet prior on ancestry proportions (same for all i)                       |
| $\lambda_{ik}^{(1)}$ | Hyperparameter of  Dirichlet posterior on ancestry proprtions for individual i                    |

* Models
:PROPERTIES:
:ID:       66e1ee52-b46d-4ce8-90bb-dd7e7b855d5a
:END:
#+latex: \label{sec:models}

** No Admixture model
In the basic mixture modelling problem, n items {X_i,\ldots,X_n} are
observed and each is assumed to belong to one of K groups. Each item
X_i has an unknown label Z_i \in {1,\ldots,K} indicating the group to
which it belongs. The main objectives are to learn about the values of
these labels, and about the value of K. Group k is characterised by a
probability distribution f(x;\phi_k), and inference for K and Z
typically also requires inference for the parameters \phi_k.

The ``No-Admixture Model'' of \citet{Pritchard_et_al_2000} is an
example of this class of models: X_i is a data set of multilocus
genotypes for individual i and the "groups" can be thought of as
idealized biological populations from which the study individuals have
ancestry. These populations are fully characterised by the allele
frequencies at each locus. Hardy-Weinberg and linkage equilibrium are
assumed so that the data for an individual from population k would be
simulated from the prior by drawing alleles from the allele frequency
distribution for population k, independently across chromosomes and
loci. The prior distributions of the allele frequencies are Dirichlet
with hyperparameters \alpha^{(0)}, independently across loci and
groups. The following algorithm simulates a data set from this model.

#+begin_latex
\begin{algorithm}{No Admixture Model: Simulation}{n,L,K,\pi,\alpha}
  \begin{FOR}{i \= 1 \TO n}
    Z_i \sim \pi
  \end{FOR}\\
  \begin{FOR}{l \= 1 \TO L}
    \begin{FOR}{k \= 1 \TO K}
      P_{lk} \sim Dirichlet(\alpha)
    \end{FOR}\\
    \begin{FOR}{i \= 1 \TO n}
      \begin{FOR}{a \= 1 \TO 2}
        X_{ila} \sim P_{lZ_i}
      \end{FOR}\\
    \end{FOR}\\
  \end{FOR}\\
\end{algorithm}
#+end_latex

See PSD for further details.

** Admixture model
An important early contribution to the literature on mixture models
for studying population structure (references above) is the
``Admixture Model'' (AM) introduced by \citet{Pritchard_et_al_2000},
in which alleles at different loci or on different chromosomes within
a single individual may have been inherited from different
groups. Thus the integer-valued labels Z_i become integer-valued
matrices in which Z_ila is the label of the group from which the
allele on chromosome a at locus l in individual i was inherited. In
general it is not possible to infer the values of the Z_ila; instead
we are interested in the genome-wide proportion Q_ik of ancestry for
individual i in group k.  The prior distributions of the ancestry
proportions are Dirichlet with hyperparameters \lambda^{(0)},
independently across individuals, and the prior for the allele
frequencies is the same as in the No Admixture Model. The following
algorithm simulates a data set from the Admixture Model model.

=FIXME: \pi or \lambda?=

#+begin_src python
for each individual i:
    Q_i \sim Dirichlet(\pi)

for each locus l:
    for each group k:
        P_lk \sim Dirichlet(\alpha^{(0)})
for each individual i:
    for each chromosome a:
        Z_ila \sim Q_i
        X_ila \sim P_{l Z_{ila}}
#+end_src
        
#+begin_latex
\begin{algorithm}{Admixture Model: Simulation}{n,K,L,\pi,\alpha^{(0)}}
  \begin{FOR}{i \= 1 \TO n}
    Q_i \sim Dirichlet(\pi)
  \end{FOR}\\
  \begin{FOR}{l \= 1 \TO L}
    \begin{FOR}{k \= 1 \TO K}
      P_{lk} \sim Dirichlet(\alpha^{(0)})
    \end{FOR}\\   
    \begin{FOR}{i \= 1 \TO n}
      \begin{FOR}{a \= 1 \TO 2}
        Z_{ila} \sim Q_i\\
        X_{ila} \sim P_{l Z_{ila}}
      \end{FOR}\\
    \end{FOR}\\                         
  \end{FOR}\\                           
\end{algorithm}
#+end_latex

** F model
The F model of \citet{Nicholson,Falush_et_al_2003} models shared
ancestry of populations by introducing an ancestral population into
the model. The populations in the mixture are characterised by allele
frequencies which depend on the frequency in the ancestral
population. The following algorithm simulates a data set under the F
model without admixture. The extension to admixture is obvious by
comparison with algorithm \ref{alg:am-sim} above.

=FIXME: move this to later?=

* Introduction to Variational Bayes
The basic idea of VB is to assume a specific parametric form for the
posterior density, and then to optimize the values of the
hyperparameters via a hill-climbing algorithm. Thus, in principle, VB
makes Bayesian posterior densities available without imposing the
computational burden of exploring the support of the posterior via a
Markov-chain sampler. Whether or not this results in a more attractive
procedure than MCMC is discussed in section \ref{sec:discussion}.

For observed data \X and unobserved parameters \phi we can write

\[
\Pr(X|K) = \frac{\Pr(\phi,X|K)}{\Pr(\phi|X,K)}  = \frac{p(\phi,X)}{q^*(\phi)},
\]

where $q^*(\phi)$ denotes the (unknown) true posterior density of
parameters \phi and $p(\phi,X)$ is the complete data likelihood (for
the purposes of this section, \phi includes the integer-valued
membership indicators \Z, as well as the real-valued parameters \P and
\Q). Taking logs and integrating with respect to some distribution
$q(\phi)$ (this will be the approximate posterior density, and in
practice it will be chosen to have a convenient parametric form) gives

#+begin_src latex
  \begin{align*}
    \log \Pr(X|K)
    &=~ \int \log p(\phi,X) q(\phi) d\phi - \int \log q^*(\phi) q(\phi) d\phi \\
    &=~ \int \log \frac{p(\phi,X)}{q(\phi)} q(\phi) d\phi - \int \log \frac{q^*(\phi)}{q(\phi)} q(\phi) d\phi \\
    &= F(q,p) + d_{KL}(q ~||~ q^*).
  \end{align*}
#+end_src
The first term $F(q, p)$ is a functional of the approximate posterior
$q$ and the complete data likelihood \p, and the second term is the
Kullback-Leibler divergence between $q(\phi)$ and the unknown true
posterior $q^*(\phi)$. While the first term can be evaluated, the
second cannot. Since $\Pr(X|K)$ is a constant, maximizing $F(q,p)$
corresponds to minimizing a reasonable measure of the distance between
the approximate posterior and the true posterior. The next section
describes hill-climbing algorithms at each iteration of which an
increase in the value of $F(q,p)$ is guaranteed. When these algorithms
reach convergence, the final value of $q$ can be used as an
approximation of the true posterior density. Furthermore, since the
maximum value of $F(q,p)$ approximates $\log \Pr(X|K)$, the posterior
distribution of the number \K of mixture components can be
investigated by fitting the model at several different values of \K.

=FIXME: In the next section we switch back to separating the
integer-valued Z from the real-valued \phi=

* Fitting the models via Variational Bayes
:PROPERTIES:
:ID:       5e73e48a-3c1d-401a-85d4-af55e59c8dde
:END:
#+latex: \label{sec:model-fitting}
\citet{Pritchard_et_al_2000} and \citet{Falush_et_al_2003} described
how to fit the above models using MCMC. In this section I describe
how to fit these models using Variational Bayes (VB).

The VB algorithms bear a strong similarity to Expectation-Maximization
(EM) algorithms, and a simple heuristic description is that both
methods work by iterating the following steps:

1. E step :: Compute the discrete probability distribution \Pr(Z|X)
             on the unknown cluster indicators, using the current parameter
             estimates.
2. M step :: Use the current distribution \Pr(Z|X) to update the
             parameter estimates.

In EM, the E step is accomplished straightforwardly using Bayes rule
and current point estimates of the parameters P and Q. In contrast, in
VB the term "parameters" in the above refers to hyperparameters
\alpha^{(1)} and \lambda^{(1)} of the posterior density, and the E
step is accomplished by averaging over the current posterior densities
for \P and \Q. \citet{Somepeople} show that this is achieved by the
following update scheme:

1. E Step ::
   Set $q(Z) \propto \exp\left\{\E_{q(\phi)} \log p(Z,X|\phi)\right\}$
2. M Step ::
   Set $q(\phi) \propto \Pr(\phi) \exp\left\{\E_{q(Z)} \log p(Z,X|\phi)\right\}$

In the following sections I focus on the E step, the M step and
assessing convergence. In all cases the full algorithm is of the
following form:

1. Set parameters (EM) / hyperparameters (VB) to their initial values.
2. E step
3. M step
4. Stop if converged, otherwise go to (2).
 
** No Admixture model
In this case the parameters are P (allele frequencies) and $\pi$
(cluster intensities). We specify that the approximate posterior
density $q(Z,\pi,P)$ can be factorised as $q(Z)q(\pi)q(\mu)$ and that
each of these three components has the same parametric form as the
prior, differing only in the hyperparameters. In other words, we
specify that q(\pi) is
Dirichlet(\lambda^{1}_{1},\ldots,\lambda^{1}_{K}), and that
$q(P_{lk\cdot}) is
Dirichlet(\alpha^{1}_{lk1},\ldots,\alpha^{1}_{lkJ_{l}})$,
independently for all \l, \k.

=FIXME: what are we saying about q(Z)=
=FIXME: how is the notation going to differentiate among these different q distributions?=

\begin{itemize}
\item Let $\gamma^{i}_{k} = q(z_{i}=k)$
\end{itemize}

*** E Step
**** EM
***** Version 1
For each $(i,k)$ compute
#+begin_src latex
\begin{align*}
  \Pr(Z_{i} = k| X_i=x) &\propto \Pr(Z_{i}=k)\Pr(X_{i}=x|Z_{i}=k) \\
  &= \pi_{k}\prod_{l}\prod_{a=1}^{2}P_{lkx_{ila}}
\end{align*}
#+end_src
**** VB
***** Version 1
#+begin_src latex
  for each $(i,k)$ compute 
  \[
  \tilde \Pr(Z_{i} = k| X_{i}) = \exp\{\E_{q(P,\pi)} ~ \log \Pr(Z_{i}|X_{i},P,\pi)\}.
  \]
#+end_src
  I.e. compute the same quantity as in the EM algorithm, but
  log-averaged over the (current) posterior densities of P and
  \pi, rather than using (current) point estimates.
***** Version 2
Using the current distribution $q(\theta)$, set

$q(z) \propto \exp\left\{\E_{q(\theta)} \log p(z,x|\theta)\right\}$.

Since $p(z,x|\theta) = \prod_{i} p(z_{i},x_{i}|\theta)$ this is done
independently for each $i$, and the E step comprises the following
algorithm:

1. For each \i
   1. For each \k
      - Compute $\gamma^{i}_{k} = \exp\left\{\E_{q(\theta)} \log p(z_{i}=k,x_{i}|\theta)\right\}$
   2. For each \k
      - $\gamma^{i}_{k} \leftarrow $\gamma^{i}_{k} / \sum_{k'} $\gamma^{i}_{k'}

#+begin_latex
\begin{algorithm}{E step}{}
  \begin{FOR}{i \= 1 \TO n}
    \begin{FOR}{k \= 1 \TO K}
      \gamma^{i}_{k} \leftarrow \exp\left\{\E_{q(\theta)} \log p(z_{i}=k,x_{i}|\theta)\right\}
    \end{FOR}\\
    \begin{FOR}{k \= 1 \TO K}
      \gamma^{i}_{k} \leftarrow \gamma^{i}_{k} / \sum_{k'} \gamma^{i}_{k'}
    \end{FOR}\\
  \end{FOR}\\
\end{algorithm}
#+end_latex



It is shown in appendix \ref{sec:appendix-nam-E} that
\begin{equation*}
\log \gamma^{i}_{k} = \digamma\Big(\lambda^{1}_{k}\Big) - \digamma\Big(\sum_{k'}\lambda^{1}_{k'}\Big) + \sum_{l} \left[\sum_{a=1}^{2} \digamma\Big(\alpha^{1}_{klx_{lia}}\Big)\right] - 2\digamma\Big(\sum_{j'=1}^{J_{l}}\alpha^{1}_{klj'}\Big).
\end{equation*}
where $\digamma$ is the digamma function.

*** M step
**** EM
***** Version 1
Use $\Pr(Z|X)$ to estimate $\mu$ and $\pi$ in the natural
  way. I.e. the cluster intensities are estimated by
#+begin_src latex
\[
\pi_{k} \leftarrow \frac{1}{n}\sum_{i}\Pr(Z_{i}=k),
\]
and the allele frequencies are estimated by
\[
P_{lkj} \leftarrow \frac{\sum_{i,a}I(X_{ila}=j)\Pr(Z_{i}=k)}{\sum_{i,a}\Pr(Z_{i}=k)}
\]
#+end_src
**** VB
***** Version 1
Use $\tilde \Pr(Z|X)$ to update the posterior densities of
P and $\pi$. This turns out to be a standard dirichlet-multinomial
update in which the hyperparameters of the posterior are the sum of
`prior counts' and `expected counts', with the latter formed using the
distribution $\tilde \Pr(Z|X)$.

***** Version 2

Using the current distribution $p(z)$, the M step comprises setting

#+begin_latex 
\begin{eqnarray*}
  q(\theta) &\propto& p(\theta)\exp\left\{\E_{q(z)} \log p(z,x|\theta)\right\} \\
  &=& 
  p(\pi)\exp\left\{\E_{q(z)} \log p(z|\pi)\right\} \times 
  p(\mu)\exp\left\{\E_{q(z)} \log p(x|\mu,z)\right\},
\end{eqnarray*}
#+end_latex

so the updates for $q(\pi)$ and $q(\mu)$ can be performed separately, by setting

#+begin_latex 
\begin{equation*}
q(\pi) \propto p(\pi)\exp\left\{\E_{q(z)} \log p(z|\pi)\right\}
\text{~~~~and~~~~}
q(\mu) \propto p(\mu)\exp\left\{\E_{q(z)} \log p(x|\mu,z)\right\}.
\end{equation*}
#+end_latex

**** Updating the approximate posterior on mixing proportions
The hyperparameters of $q(\pi)$ are updated according to the following
algorithm (appendix \ref{sec:appendix-nam-M-pi}):

#+begin_latex 
\begin{itemize}
\item For each population $k$
  \begin{itemize}
  \item Calculate the approximate posterior expected count of individuals assigned to population $k$: $n_{k} = \sum_{i}\gamma^{i}_{k}$
  \item Set $\lambda^{1}_{k} \leftarrow \lambda^{0}_{k} + n_{k}$
  \end{itemize}
\end{itemize}
#+end_latex

#+begin_latex
%% \begin{algorithmic}
%% \FORALL{groups $k$}
%%     \STATE $n_{k} \gets \sum_{i}\gamma^{i}_{k}$
%%     \STATE $\lambda^{1}_{k} \gets \lambda^{0}_{k} + n_{k}$
%% \ENDFOR
%% \end{algorithmic}
#+end_latex

**** Updating the approximate posterior on allele frequencies
The hyperparameters of $q(\mu)$ are updated according to the following
algorithm (see appendix \ref{sec:appendix-nam-M-P}):

#+begin_latex 
For each locus $l$
\begin{itemize}
\item For each population $k$
  \begin{itemize}
  \item For each allele $j$
    \begin{itemize}
    \item Calculate the approximate posterior expected count of alleles of type $j$ generated by population $k$ at locus $l$: $n_{lkj} = \sum_{i} \sum_{a}\gamma^{i}_{k}I(x_{lia}=j)$
    \item Set $\alpha^{1}_{lkj} \leftarrow \alpha^{0}_{lkj} + n_{lkj}$
    \end{itemize}
  \end{itemize}
\end{itemize}
#+end_latex

*** Monitoring convergence

The E and M steps are iterated until the increase in $F(q,p)$ is
sufficiently small that convergence is judged to have been reached,
which means that it is necessary to evaluate $F(q,p)$ at the end of
each iteration. Since $q()$ factorises by assumption/definition,

#+begin_latex 
\begin{align*}
  F(q,p) 
  &=~ \int q(\theta)q(z)\log \frac{p(\theta)p(z,x|\theta)}{q(\theta)q(z)} d\theta dz\\
  &=~ \int q(\theta)\log \frac{p(\theta)}{q(\theta)} d\theta + \int q(\theta)q(z)\log \frac{p(z,x|\theta)}{q(z)} d\theta dz\\
  &=~ -d_{KL}(q||p) + \E_{q(\pi,z)}\log p(z|\pi) + \E_{q(\mu,z)} \log p(x|z,\mu) + H\(q(z)\),\\
\end{align*}
#+end_latex

where $H\(q(z)\) = -\int q(z)\log q(z) dz$ is the Shannon entropy of
$q(z)$. Computation of these four terms is described in Appendix
\ref{sec:appendix-nam-convergence} .
*** EM algorithm
#+begin_latex
%% \begin{algorithm}{No Admixture: EM algorithm}{n,K,L,X}
%%   \\\\
%%   \text{E step}\\\\
%%   \begin{FOR}{\EACH i \IN 1 \ldots n}
%%     \begin{FOR}{\EACH k \IN 1 \ldots K}
%%       \gamma_i^k = \pi_{k}\prod_{l}\prod_{a=1}^{2}P_{lkX_{ila}}
%%     \end{FOR}
%%   \end{FOR}
%%   \\\\
%%   \text{M step}\\\\
%%   \begin{FOR}{\EACH k \IN 1 \ldots K}
%%       \pi_{k} \leftarrow \frac{1}{n}\sum_{i} \gamma_i^k\\
%%       \begin{FOR}{\EACH l \IN 1 \ldots L}
%%         \begin{FOR}{\EACH j \IN 1 \ldots J_L}
%%           P_{lkj} \leftarrow \frac{\sum_{i,a}I(X_{ila}=j)\Pr(Z_{i}=k)}{\sum_{i,a}\Pr(Z_{i}=k)}
%%         \end{FOR}
%%       \end{FOR}
%%   \end{FOR}
%% \end{algorithm}

%% \begin{lstlisting}[mathescape]
%% for i = 1,...,n
%%     x $\leftarrow e^{i\pi}$
%% \end{lstlisting}
#+end_latex

#+begin_latex
\begin{algorithm}
  \caption{EM algorithm}
  \label{EM algorithm}
  \begin{lstlisting}[mathescape]

    E step

    for $i$ in $1, \ldots, n$
        for $k$ in $1, \ldots, K$
            $\gamma_i^k \gets \pi_{k}\prod_{l}\prod_{a=1}^{2}P_{lkX_{ila}}$

    M step

    for $k$ in $1 \ldots K$
        $\pi_{k} \gets \frac{1}{n}\sum_{i} \gamma_i^k$
        for $l$ in $1, \ldots, L$
            for $j$ in $1,\ldots,J_L$
                $P_{lkj} \gets \frac{\sum_{i,a}I(X_{ila}=j)\Pr(Z_{i}=k)}{\sum_{i,a}\Pr(Z_{i}=k)}$

  \end{lstlisting}
\end{algorithm}
#+end_latex


** Admixture model
In the Admixture model, the unobserved quantities are \Z, \Q and
\P. As in the No Admixture model, we specify that approximate
posterior density $q(Z,Q,P)$ can be factorised as $q(Z)q(Q)q(P)$ and
that each of these three components has the same parameteric form as
in the prior, differing only in the hyperparameters. Specifically, we
specify that the ancestry vectors $q(Q_{i\cdot})$ are each
Dirichlet(\lambda^{1}_{i1},\ldots,\lambda^{1}_{iK})$ and, as in the No
Admixture model, that $q(P_{lk\cdot}) is
Dirichlet(\alpha^{1}_{lk1},\ldots,\alpha^{1}_{lkJ_{l}})$,
independently for all \l, \k.

\begin{itemize}
\item Let $\gamma^{ila}_{k} = q(z_{ila}=k)$
\end{itemize}

*** E step
Using the current distribution $q(\theta)$, set $q(z)$ proportional to

$\exp\left\{\E_{q(\theta)} \log p(z,x|\theta)\right\}$.

Since

$p(z,x|\theta) = \prod_{i} \prod_{l} \prod_{a=1}^{2}p(z_{ila},x_{ila}|\theta)$

this is done independently for each $(i,l,a)$, and the E step
comprises the following algorithm:

#+begin_latex
\begin{itemize}
\item For each $(i,l,a)$
  \begin{itemize}
  \item For each $k$
    \begin{itemize}
    \item compute $\gamma^{ila}_{k} = \exp\left\{\E_{q(\theta)} \log p(z_{ila}=k,x_{ila}|\theta)\right\}$
    \end{itemize}
  \item renormalise the $\gamma^{ila}_{\cdot}$
  \end{itemize}
\end{itemize}
#+end_latex

In Appendix \ref{sec:appendix-am-E} it is shown that
\begin{equation*}
\log \gamma^{ila}_{k} = \digamma\Big(\lambda^{1}_{ik}\Big) - \digamma\Big(\sum_{k'}\lambda^{1}_{ik'}\Big) + \digamma\Big(\alpha^{1}_{klx_{lia}}\Big) - \digamma\Big(\sum_{j'=1}^{J_{l}}\alpha^{1}_{klj'}\Big),
\end{equation*}
where $\digamma$ is the digamma function.

*** M step
Using the current distribution $p(z)$, the M step involves setting
$q(\theta)$ proportional to

#+begin_latex
\begin{eqnarray*}
  p(\theta)\exp\left\{\E_{q(z)} \log p(z,x|\theta)\right\} \\
  &=& 
  p(\pi)\exp\left\{\E_{q(z)} \log p(z|\pi)\right\} \times 
  p(\mu)\exp\left\{\E_{q(z)} \log p(x|\mu,z)\right\},
\end{eqnarray*}
#+end_latex

and so the updates for $q(\pi)$ and $q(\mu)$ can be performed
separately, by setting

#+begin_latex
\begin{equation*}
  q(\pi) \propto p(\pi)\exp\left\{\E_{q(z)} \log p(z|\pi)\right\}
  \text{~~~~and~~~~}
  q(\mu) \propto p(\mu)\exp\left\{\E_{q(z)} \log p(x|\mu,z)\right\}.
\end{equation*}
#+end_latex

**** Updating the approximate posterior on admixture proportions
The hyperparameters of $q(\pi)$ are updated according to the following
algorithm (see Appendix \ref{sec:appendix-am-M-pi}):

#+begin_latex
\begin{itemize}
\item For each individual $i$
  \begin{itemize}
  \item For each population $k$
    \begin{itemize}
    \item Calculate the approximate posterior expected count of alleles in individual $i$ assigned to population $k$: $m_{ik} = \sum_{l} \sum_{a=1}^{2}\gamma^{ila}_{k}$
    \item Set $\lambda^{1}_{ik} \leftarrow \lambda^{0}_{ik} + m_{ik}$.
    \end{itemize}
  \end{itemize}
\end{itemize}
#+end_latex

**** Updating the approximate posterior on allele frequencies
The hyperparameters of $q(\mu)$ are updated according to the following
algorithm (see Appendix \ref{sec:appendix-am-M-P}):

#+begin_latex
For each locus $l$
\begin{itemize}
\item For each population $k$
  \begin{itemize}
  \item For each allele $j$
    \begin{itemize}
    \item Calculate the approximate posterior expected count of alleles of type $j$ generated by population $k$ at locus $l$: $m_{lkj} = \sum_{i} \sum_{a}\gamma^{ila}_{k}I(x_{lia}=j)$
    \item Set $\alpha^{1}_{lkj} \leftarrow \alpha^{0}_{lkj} + n_{lkj}$
    \end{itemize}
  \end{itemize}
\end{itemize}
#+end_latex

*** Monitoring convergence
Since $q()$ factorises by definition,

#+begin_latex
\begin{align*}
  F(q,p) 
  &=~ \int q(\theta)q(z)\log \frac{p(\theta)p(z,x|\theta)}{q(\theta)q(z)} d\theta dz\\
  &=~ \int q(\theta)\log \frac{p(\theta)}{q(\theta)} d\theta + \int q(\theta)q(z)\log \frac{p(z,x|\theta)}{q(z)} d\theta dz\\
  &=~ -d_{KL}(q||p) + \E_{q(\pi,z)}\log p(z|\pi) + \E_{q(\mu,z)} \log p(x|z,\mu) + H\(q(z)\),\\
\end{align*}
#+end_latex

where $H\(q(z)\) = -\int q(z)\log q(z) dz$ is the Shannon entropy of
$q(z)$. Computation of these four terms is described in Appendix
\ref{sec:appendix-am-convergence}.

** Admixture model with correlated allele frequencies
The correlated frequencies model affects how we update $q(\mu)$. The E
step is unchanged, as this involves estimating $q(z)$ given the
current $q(\mu,\pi)$. In the M step, the update of $q(\pi)$ is also
unchanged, as this doesn't involve $\mu$. I think the update of
$q(\mu)$ in the correlated frequencies model differs only in that the
'prior counts' of the number of copies of allele $j$ observed in
population $k$ at locus $l$ are now given by $\alpha^{0}_{lkj}$
** Parallel algorithms
* Results
:PROPERTIES:
:ID:       6d8cbdfb-0be1-474d-8a5f-74dcecb78916
:END:
#+latex: \label{sec:results}
** Known K
** Inferring K
#+ATTR_LaTeX: width=15cm
[[file:images/vbnam-simulation-results-n80-L1000-Fpoint6-10runs.png]]
** Parallel processing
* Discussion
:PROPERTIES:
:ID:       280c42eb-52a3-46ff-9812-61a38e0b82ae
:END:
#+latex: \label{sec:discussion}

\cite{Pritchard_et_al_2000} introduced an AM for loosely linked markers in
which the ancestry labels Z_{i.a} are autocorrelated along a chromosome
due to linkage. In this situation it can be possible to estimate
Z_ila at each locus. A disadvantage of methods based on PCA is that
they are not easily extended in this manner: the principal components
are eigenvectors of a covariance matrix which is estimated by
averaging across all loci.
* Appendix
:PROPERTIES:
:ID:       5b050c13-e5a3-4561-8623-54af42c27253
:END:
#+latex: \label{sec:appendix}
** Updates in variational Bayes algorithm

*** No-admixture model
**** E step
\label{sec:appendix-nam-E}

We need to evaluate

$\gamma^{i}_{k} \propto \exp\left\{\E_{q(\theta)} \log p(z_{i}=k,x_{i}|\theta)\right\}$.

The complete-data log likelihood is

\begin{eqnarray*}
\log p(z_{i}=k,x_{i}|\theta) 
&=& \log \pi_{k} + \sum_{l}\sum_{a=1}^{2}\log p(x_{ila}|\mu_{kl\cdot}) \\
&=& \log \pi_{k} + \sum_{l}\sum_{a=1}^{2} \log \mu_{klx_{ila}},
\end{eqnarray*}

so we need to evaluate integrals of the form

$\int q(\pi) \log \pi_{k} d\pi$ and $\int q(\mu_{kl\cdot}) \log \mu_{klj} d\mu_{kl\cdot}$.

Since the distributions $q(\pi)$ and $q(\mu_{kl\cdot})$ are both
Dirichlet, these have the same form. The first is

\begin{eqnarray*}
\int q(\pi) \log \pi_{k} d\pi 
&=& \int \left[\frac{\Gamma\Big(\sum_{k'}\lambda^{1}_{k'}\Big)}{\prod_{k'}\Gamma\Big(\lambda^{1}_{k'}\Big)}\prod_{k}\pi_{k}^{\lambda^{1}_{k}-1}\right] \log \pi_{k} d\pi \\
&=& \digamma\Big(\lambda^{1}_{k}\Big) - \digamma\Big(\sum_{k'}\lambda^{1}_{k'}\Big),
\end{eqnarray*}
where $\digamma$ is the digamma function, and the second one is
\begin{equation*}
\int q(\mu_{kl\cdot}) \log \mu_{klj} d\mu_{kl\cdot} = \digamma\Big(\alpha^{1}_{klj}\Big) - \digamma\Big(\sum_{j'}\alpha^{1}_{klj'}\Big).
\end{equation*}

\paragraph{}
The expectation that we are trying to evaluate is then

\begin{eqnarray*}
\log \gamma^{i}_{k} 
&=& \E_{q(\theta)}\log p(z_{i}=k,x_{i}|\theta) \\
&=& \int q(\pi) \log \pi_{k} d\pi + \sum_{l}\sum_{a=1}^{2}\int q(\mu_{lk\cdot}) \log \mu_{lkx_{ila}} d\mu_{lk\cdot} \\
&=& \digamma\Big(\lambda^{1}_{k}\Big) - \digamma\Big(\sum_{k'}\lambda^{1}_{k'}\Big) + \sum_{l} \left[\sum_{a=1}^{2} \digamma\Big(\alpha^{1}_{klx_{lia}}\Big)\right] - 2\digamma\Big(\sum_{j'=1}^{J_{l}}\alpha^{1}_{klj'}\Big).
\end{eqnarray*}

**** M step
***** Updating the hyperparameters of $q(\pi)$
\label{sec:appendix-nam-M-pi}

We want to set $q(\pi)$ proportional to

$p(\pi)\exp\left\{\E_{q(z)} \log p(z|\pi)\right\}$.

The expectation is

\begin{eqnarray*}
\E_{q(z)} \log p(z|\pi)  = \E_{q(z)} \sum_{i} \log \pi_{z_{i}}
&=& \sum_{z_{1},\ldots,z_{n}}\sum_{i} \left[\log \pi_{z_{i}} \right] \gamma_{1z_{1}},\ldots, \gamma_{nz_{n}}\\
&=& \sum_{i} \sum_{k} \gamma^{i}_{k} \log \pi_{k} \\
&=& \sum_{k} \log \pi_{k}^{n_{k}}   \\
\end{eqnarray*}

where $n_{k} = \sum_{i} \gamma^{i}_{k}$ is the current approximate
posterior expected number of individuals assigned to population
$k$. Therefore

\begin{eqnarray*}
p(\pi)\exp\left\{\E_{q(z)} \log p(z|\pi)\right\}
&\propto& \prod_{k}\pi_{k}^{\lambda^{0}_{k} - 1 + n_{k} },
\end{eqnarray*}

and the update is achieved by setting the hyperparameters equal to the
sum of the prior counts and the current approximate posterior expected
counts:

\begin{equation*}
\lambda^{1}_{k} \leftarrow \lambda^{0}_{k} + n_{k}.
\end{equation*}

***** Updating the hyperparameters of $q(\mu)$
\label{sec:appendix-nam-M-P}

We want to set $q(\mu)$ proportional to 

$p(\mu)\exp\left\{\E_{q(z)} \log p(x|\mu,z)\right\}$.

This factorises across loci and populations as

\begin{eqnarray*}
p(\mu)\exp\left\{\E_{q(z)} \log p(x|\mu,z)\right\} 
&=& \left[\prod_{l}\prod_{k}p(\mu_{lk})\right]\exp\left\{\sum_{l} \sum_{i}\E_{q(z_{i})} \log p(x_{li\cdot}|\mu_{lz_{i}})\right\} \\
&=& \prod_{l}\left[\prod_{k}p(\mu_{lk})\right]\exp\left\{\sum_{i} \sum_{k} \gamma^{i}_{k}\log p(x_{li\cdot}|\mu_{lk})\right\} \\
&=& \prod_{l}\prod_{k}p(\mu_{lk})\exp\left\{\sum_{i} \gamma^{i}_{k}\log p(x_{li\cdot}|\mu_{lk})\right\}, \\
\end{eqnarray*}

so the approximate posterior distributions on allele frequencies can
be updated separately in each population and at each locus.

\begin{eqnarray*}
p(\mu_{lk})\exp\left\{\sum_{i} \gamma^{i}_{k}\log p(x_{li}|\mu_{lk})\right\}
&=& p(\mu_{lk})\exp\left\{\sum_{i} \gamma^{i}_{k}\sum_{a}\sum_{j}\log \mu_{lkj}^{I(x_{lia}=j)}\right\} \\
&\propto& \prod_{j}\mu_{lkj}^{\alpha^{0}_{lkj}}\exp\left\{\sum_{j} \log \mu_{lkj} \sum_{i} \sum_{a}\gamma^{i}_{k}I(x_{lia}=j)\right\} \\
&=& \prod_{j}\mu_{lkj}^{\alpha^{0}_{lkj}}\exp\left\{n_{lkj}\log \mu_{lkj}\right\},\\
\end{eqnarray*}

where $n_{lkj} = \sum_{i} \sum_{a}\gamma^{i}_{k}I(x_{lia}=j)$ is the
expected number of $j$ alleles observed at locus $l$ in population
$k$, with the expectation taken w.r.t. $q(z)$. This results in

\begin{equation*}
q(\mu_{lk}) \propto \prod_{j} \mu_{lkj}^{\alpha^{0}_{lkj} - 1 + n_{lkj}},
\end{equation*}

which is fulfilled by setting the hyperparameters equal to the sum of
the prior counts and the current approximate posterior expected
counts:

\begin{equation*}
\alpha^{1}_{lkj} \leftarrow \alpha^{0}_{lkj} + n_{lkj}.
\end{equation*}

**** Monitoring convergence
\label{sec:appendix-nam-convergence}
***** The K-L divergence between prior and approximate posterior
\label{KL-term-no-admix}

#+begin_latex 
\begin{align*}
  d_{KL}(q||p)
  =&~ \int q(\theta)\log \frac{q(\theta)}{p(\theta)} d\theta \\
  =&~ \int q(\mu) \log \frac{q(\mu)}{p(\mu)} d\mu + \int q(\pi) \log \frac{q(\pi)}{p(\pi)} d\pi\\
  =&~ \sum_{l} \sum_{k} d_{KL}\Big(q(\mu_{lk\cdot})||p(\mu_{lk\cdot})\Big) + d_{KL}\Big(q(\pi_{\cdot})||p(\pi_{\cdot})\Big),
\end{align*}
#+end_latex

in which the component densities are all Dirichlet. The K-L divergence
of two Dirichlet densities with parameters
$\alpha_{1},\ldots,\alpha_{S}$ and $\beta_{1},\ldots,\beta_{S}$ is
given in equation 52 of \citet{penny-roberts-2000} as

#+begin_latex
\begin{align*}
  d_{KL}(\text{Dir}(\mathbf \alpha) || \text{Dir}(\mathbf\beta)) = 
  \log \frac{\Gamma(\sum_{s}\alpha_{s})}{\Gamma(\sum_{s}\beta_{s})} + 
  \sum_{s} \log \frac{\Gamma(\beta_{s})}{\Gamma(\alpha_{s})} +
  \sum_{s}(\alpha_{s} - \beta_{s})\(\Psi(\alpha_{s}) - \Psi(\sum_{s}\alpha_{s})\)
\end{align*}
#+end_latex

***** The average missing data probability term
#+begin_latex
\begin{align*}
  \E_{q(\pi,z)}\log p(z|\pi) 
  =&~ \sum_{i} \E_{q(z_{i})}\E_{q(\pi_{\cdot})} \log \pi_{z_{i}} \\
  =&~ \sum_{i} \sum_{k} \gamma^{i}_{k} \int q(\pi_{\cdot}) \log \pi_{k} d\pi_{\cdot} \\
  =&~ \sum_{i} \sum_{k} \gamma^{i}_{k} \left[\digamma(\lambda^{1}_{k}) - \digamma(\sum_{k'}\lambda^{1}_{k'})\right] \\
  =&~ \left[ \sum_{i} \sum_{k} \gamma^{i}_{k} \digamma(\lambda^{1}_{k})\right] - n\digamma(\sum_{k'}\lambda^{1}_{k'})\\
  =&~ \left[ \sum_{k} m_{k} \digamma(\lambda^{1}_{ik})\right] - n\digamma(\sum_{k'}\lambda^{1}_{k'}),\\
\end{align*}
#+end_latex

where $m_{k} = \sum_{i} \gamma^{i}_{k}$ is the expected number of
individuals that derive from population $k$.

***** The average log likelihood term
#+begin_latex
\begin{align*}
  \E_{q(\mu,z)} \log p(x|z,\mu) 
  &=~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \E_{q(z_{i})} \E_{q(\mu_{lz_{i}\cdot})} \log p(x_{ila}|z_{i},\mu_{lz_{i}x_{ila}}), \\
  &=~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \sum_{k} \gamma^{i}_{k} \int q(\mu_{lk\cdot})\log \mu_{lkx_{ila}} d\mu_{lk\cdot}. \\
  &=~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \sum_{k} \gamma^{i}_{k} \left[\digamma(\alpha^{1}_{lkx_{ila}}) - \digamma(\sum_{j}\alpha^{1}_{lkj})\right]\\
  &=~ \sum_{l} \sum_{k} \sum_{j} \left[\digamma(\alpha^{1}_{lkj}) - \digamma(\sum_{j'}\alpha^{1}_{lkj'})\right] \sum_{i} \sum_{a=1}^{2} \gamma^{i}_{k}I(x_{ila}=j) \\
  &=~ \sum_{l} \sum_{k} \sum_{j} \left[\digamma(\alpha^{1}_{lkj}) - \digamma(\sum_{j'}\alpha^{1}_{lkj'})\right] m_{lkj}, \\
  \intertext{where $m_{lkj} = \sum_{i} \sum_{a=1}^{2} \gamma^{i}_{k}I(x_{ila}=j)$ is the expected number of alleles of type $j$ at locus $l$ that derive from population $k$.}
  &=~ \sum_{l} \sum_{k} \left[\sum_{i}\gamma^{i}_{k}\sum_{a=1}^{2}\digamma(\alpha^{1}_{lkx_{ila}})\right] - n\digamma(\sum_{j'}\alpha^{1}_{lkj'})
\end{align*}
#+end_latex
***** The entropy of the probability distribution over the missing indicators
#+begin_latex
\begin{align*}
  H\(q(z)\) 
  &=~ -\E_{q(z)} \log q(z) \\
  &=~ -\sum_{i} \sum_{k} \gamma^{i}_{k} \log \gamma^{i}_{k}\\
\end{align*}
#+end_latex
*** Admixture model
**** E step
\label{appendix-am-E}
We need to evaluate

$\gamma^{ila}_{k} \propto \exp\left\{\E_{q(\theta)} \log p(z_{ila}=k,x_{ila}|\theta)\right\}$.

The complete-data log likelihood is

\begin{equation*}
\log p(z_{ila}=k,x_{ila}|\theta) = \log \pi_{ik} + \log \mu_{klx_{ila}},
\end{equation*}

so we need to evaluate integrals of the form

$\int q(\pi_{i\cdot}) \log \pi_{ik} d\pi_{i\cdot}$ and $\int q(\mu_{kl\cdot}) \log \mu_{klj} d\mu_{kl\cdot}$.

Since the distributions $q(\pi_{i\cdot})$ and $q(\mu_{kl\cdot})$ are
both Dirichlet, these have the same form. The first is

\begin{eqnarray*}
\int q(\pi_{i\cdot}) \log \pi_{ik} d\pi_{i\cdot} 
&=& \int \left[\frac{\Gamma\Big(\sum_{k'}\lambda^{1}_{ik'}\Big)}{\prod_{k'}\Gamma\Big(\lambda^{1}_{ik'}\Big)}\prod_{k'}\pi_{ik'}^{\lambda^{1}_{ik}-1}\right] \log \pi_{ik} d\pi_{i\cdot} \\
&=& \digamma\Big(\lambda^{1}_{ik}\Big) - \digamma\Big(\sum_{k'}\lambda^{1}_{ik'}\Big),
\end{eqnarray*}
where $\digamma$ is the digamma function, and the second one is
\begin{equation*}
\int q(\mu_{kl\cdot}) \log \mu_{klj} d\mu_{kl\cdot} = \digamma\Big(\alpha^{1}_{klj}\Big) - \digamma\Big(\sum_{j'}\alpha^{1}_{klj'}\Big).
\end{equation*}

\paragraph{}
The expectation that we are trying to evaluate is then

\begin{eqnarray*}
\log \gamma_{ilk} 
&=& \E_{q(\theta)}\log p(z_{il}=k,x_{il}|\theta) \\
&=& \int q(\pi_{i\cdot}) \log \pi_{ik} d\pi_{i\cdot} + \int q(\mu_{lk\cdot}) \log \mu_{lkx_{ila}} d\mu_{lk\cdot} \\
&=& \digamma\Big(\lambda^{1}_{ik}\Big) - \digamma\Big(\sum_{k'}\lambda^{1}_{ik'}\Big) + \digamma\Big(\alpha^{1}_{klx_{lia}}\Big) - \digamma\Big(\sum_{j'=1}^{J_{l}}\alpha^{1}_{klj'}\Big).
\end{eqnarray*}

**** M step
***** Updating the hyperparameters of $q(\pi)$
\label{sec:appendix-am-M-pi}

We want to set $q(\pi)$ proportional to

$p(\pi)\exp\left\{\E_{q(z)} \log p(z|\pi)\right\}$.

This factorises across individuals as

\begin{equation*}
p(\pi)\exp\left\{\E_{q(z)} \log p(z|\pi)\right\} = \prod_{i} p(\pi_{i\cdot})\exp\left\{\E_{q(z_{i\cdot\cdot})} \log p(z_{i\cdot\cdot}|\pi)\right\},
\end{equation*}

so we can update the hyperparameters of $p(\pi_{i\cdot})$
independently for each individual $i$. The expectation is

\begin{eqnarray*}
\E_{q(z_{i\cdot\cdot})} \log p(z_{i\cdot\cdot}|\pi)  &=& \E_{q(z\cdot\cdot)} \sum_{l} \sum_{a=1}^{2} \log \pi_{iz_{ila}} \\
&=& \sum_{l} \sum_{a=1}^{2} \sum_{k} \gamma^{ila}_{k} \log \pi_{ik} \\
&=& \sum_{k} \left[\log \pi_{ik}\right] \sum_{l} \sum_{a=1}^{2} \gamma^{ila}_{k} \\
&=& \sum_{k} \log \pi_{ik}^{m_{ik}} \\
\end{eqnarray*}

where $m_{ik} = \sum_{l} \sum_{a=1}^{2} \gamma^{ila}_{k}$ is the
current approximate posterior expected number of allele copies at all
loci in individual $i$ that derive from population $k$. Therefore

\begin{eqnarray*}
p(\pi_{i\cdot})\exp\left\{\E_{q(z_{i\cdot\cdot})} \log p(z_{i\cdot\cdot}|\pi_{i\cdot})\right\}
&\propto& \prod_{k}\pi_{ik}^{\lambda^{0}_{ik} - 1 + m_{ik} },
\end{eqnarray*}

and the update is achieved by setting the hyperparameters equal to the
sum of the prior counts and the current approximate posterior expected
counts:

\begin{equation*}
\lambda^{1}_{ik} \leftarrow \lambda^{0}_{ik} + m_{ik}.
\end{equation*}

=FIXME: equal to a distribution proportional to=

***** Updating the hyperparameters of $q(\mu)$
\label{sec:appendix-am-M-P}

We want to set $q(\mu)$ proportional to

$p(\mu)\exp\left\{\E_{q(z)} \log p(x|\mu,z)\right\}$.

This factorises across loci and populations as

\begin{eqnarray*}
p(\mu)\exp\left\{\E_{q(z)} \log p(x|\mu,z)\right\} 
&=& \left[\prod_{l}\prod_{k}p(\mu_{lk})\right]\exp\left\{\sum_{l} \sum_{i} \sum_{a=1}^{2}\E_{q(z_{i})} \log p(x_{ila}|\mu_{lz_{i}})\right\} \\
&=& \prod_{l}\left[\prod_{k}p(\mu_{lk})\right]\exp\left\{\sum_{i} \sum_{a=1}^{2}\sum_{k} \gamma^{ila}_{k}\log p(x_{ila}|\mu_{lk})\right\} \\
&=& \prod_{l}\prod_{k}p(\mu_{lk})\exp\left\{\sum_{i} \sum_{a=1}^{2} \gamma^{ila}_{k}\log p(x_{ila}|\mu_{lk})\right\}, \\
\end{eqnarray*}

so the approximate posterior distributions on allele frequencies can
be updated separately in each population and at each locus.

\begin{eqnarray*}
p(\mu_{lk})\exp\left\{\sum_{i} \sum_{a=1}^{2} \gamma^{ila}_{k}\log p(x_{ila}|\mu_{lk})\right\}
&=& p(\mu_{lk})\exp\left\{\sum_{i} \sum_{a=1}^{2} \gamma^{ila}_{k} \sum_{j} \log \mu_{lkj}^{I(x_{lia}=j)}\right\} \\
&\propto& \prod_{j}\mu_{lkj}^{\alpha^{0}_{lkj}-1}\exp\left\{\sum_{j} \left[\log \mu_{lkj}\right] \sum_{i} \sum_{a}\gamma^{ila}_{k}I(x_{lia}=j)\right\}\\
&=& \prod_{j}\mu_{lkj}^{\alpha^{0}_{lkj}-1+m_{lkj}},\\
\end{eqnarray*}

where $m_{lkj} = \sum_{i} \sum_{a}\gamma^{ila}_{k}I(x_{ila}=j)$ is the
expected number of $j$ alleles observed at locus $l$ in population
$k$, with the expectation taken w.r.t. $q(z)$. The update is therefore
achieved by setting

\begin{equation*}
\alpha^{1}_{lkj} \leftarrow \alpha^{0}_{lkj} + m_{lkj}.
\end{equation*}

**** Monitoring convergence
\label{sec:appendix-am-convergence}
***** The K-L divergence between prior and approximate posterior
This is similar to the no-admixture case (section
\ref{sec:appendix-nam-convergence}); whereas $\pi$ previously
comprised a single distribution over $\{1,\ldots,K\}$, it now
comprises $n$ such distributions:

#+begin_latex
\begin{align*}
  d_{KL}(q||p)
  =&~ \sum_{l} \sum_{k} d_{KL}\Big(q(\mu_{lk\cdot})||p(\mu_{lk\cdot})\Big) + \sum_{i} d_{KL}\Big(q(\pi_{i\cdot})||p(\pi_{i\cdot})\Big),
\end{align*}
#+end_latex

in which the component densities are all Dirichlet. 

***** The average missing data probability term

#+begin_latex
\begin{align*}
  \E_{q(\pi,z)}\log p(z|\pi) 
  =&~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \E_{q(z_{ila})}\E_{q(\pi_{i\cdot})} \log \pi_{iz_{ila}} \\
  =&~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \sum_{k} \gamma^{ila}_{k} \int q(\pi_{i\cdot}) \log \pi_{ik} d\pi_{i\cdot} \\
  =&~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \sum_{k} \gamma^{ila}_{k} \left[\digamma(\lambda^{1}_{ik}) - \digamma(\sum_{k'}\lambda^{1}_{ik'})\right] \\
  =&~ \sum_{i} \left[ \sum_{l} \sum_{a=1}^{2} \sum_{k} \gamma^{ila}_{k} \digamma(\lambda^{1}_{ik})\right] - 2L\digamma(\sum_{k'}\lambda^{1}_{ik'})\\
  =&~ \sum_{i} \left[ \sum_{k} m_{ik} \digamma(\lambda^{1}_{ik})\right] - 2L\digamma(\sum_{k'}\lambda^{1}_{ik'}),\\
\end{align*}
#+end_latex

where $m_{ik} = \sum_{l} \sum_{a=1}^{2} \gamma^{ila}_{k}$ is the
expected number of allele copies in individual $i$ that derive from
population $k$.

***** The average log likelihood term
#+begin_latex
\begin{align*}
  \E_{q(\mu,z)} \log p(x|z,\mu) 
  &=~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \E_{q(z_{ila})} \E_{q(\mu_{lz_{ila}\cdot})} \log p(x_{ila}|z_{ila},\mu_{lz_{ila}x_{ila}}), \\
  &=~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \sum_{k} \gamma^{ila}_{k} \int q(\mu_{lk\cdot})\log \mu_{lkx_{ila}} d\mu_{lk\cdot}. \\
  &=~ \sum_{l} \sum_{i} \sum_{a=1}^{2} \sum_{k} \gamma^{ila}_{k} \left[\digamma(\alpha^{1}_{lkx_{ila}}) - \digamma(\sum_{j}\alpha^{1}_{lkj})\right]\\
  &=~ \sum_{l} \sum_{k} \sum_{j} \left[\digamma(\alpha^{1}_{lkj}) - \digamma(\sum_{j'}\alpha^{1}_{lkj'})\right] \sum_{i} \sum_{a=1}^{2} \gamma^{ila}_{k}I(x_{ila}=j) \\
  &=~ \sum_{l} \sum_{k} \sum_{j} \left[\digamma(\alpha^{1}_{lkj}) - \digamma(\sum_{j'}\alpha^{1}_{lkj'})\right] m_{lkj}, \\
\end{align*}
#+end_latex
where $m_{lkj} = \sum_{i} \sum_{a=1}^{2} \gamma^{ila}_{k}I(x_{ila}=j)$
is the expected number of alleles of type $j$ at locus $l$ that derive
from population $k$.

***** The entropy of the probability distribution over the missing indicators
#+begin_latex
\begin{align*}
  H\(q(z)\) 
  &=~ -\E_{q(z)} \log q(z) \\
  &=~ -\sum_{l}\sum_{i}\sum_{a=1}^{2} \sum_{k} \gamma^{ila}_{k} \log \gamma^{ila}_{k}\\
\end{align*}
#+end_latex

** EM algorithm update for $\mu$ in correlated frequencies model

\paragraph{}
The complete-data posterior density (assuming a flat prior on $q$) is

#+begin_latex 
\begin{align*}
p(\theta|x,z) = p(\mu,q|x,z) \propto&~ p(\mu)p(q)p(z|q)p(x|z,\mu)                                                                     \\
=&\prod_l  \( \prod_k p(\mu_{lk}) \) \( \prod_i p(z_{li}|q_{iz_{li}})p(x_{li}|\mu_{lz_{li}}) \),                                    \\
=&\prod_l  \( \prod_k p(\mu_{lk}) \) \( \prod_i q_{iz_{li}}p(x_{li}|\mu_{lz_{li}}) \),                                         \\
\intertext{so the complete-data log posterior (up to an additive constant) is}
\log p(\theta|x, z) =& \sum_l \( \sum_k \log p(\mu_{lk}) \) + \( \sum_i \log \Big( q_{iz_{li}}p(x_{li}|\mu_{lz_{li}}) \Big) \),
\intertext{the expectation of which (with respect to the current distribution on the missing data $z$) is}
\E_{z|x,\theta^*}\log p(\theta|x, z)
=& \sum_l \sum_k \log p(\mu_{lk}) + \sum_l \sum_k\sum_i \log \Big( \gamma_{ik}p(x_{li}|\mu_{lk}) \Big)p_{\theta^*}(k\|x_{li})  \\
=& \sum_l \sum_k \log p(\mu_{lk}) + \sum_l \sum_k\sum_i \(\log \gamma_{ik}\)p_{\theta^*}(k\|x_{li}) \\~~~~~~~~~~~~~~~&+ \sum_l \sum_k\sum_i \Big( \log p(x_{li}|\mu_{lk}) \Big)p_{\theta^*}(k\|x_{li}).
\intertext{With ancestral allele frequency $\alpha_l$ at locus $l$, and a Beta$(\alpha_lF_k',(1-\alpha_l)F_k')$ prior on the frequency in population $k$ ($F_k' = \frac{1-F_k}{F_k}$), and a Bernoulli likelihood, this is}
\sum_l \sum_k \log \( \mu_{lk}^{\alpha F_k'-1}(1-\mu_{lk})^{(1-\alpha_k)F_k' - 1} \) &+ \sum_l \sum_k\sum_i \(\log \gamma_{ik}\)p_{\theta^*}(k\|x_{li})\\ &+ \sum_l \sum_k\sum_i  \log \Big(\mu_{lk}^{x_{li}}(1-\mu_{lk})^{(1-x_{li})} \Big)p_{\theta^*}(k\|x_{li}).
\end{align*}
#+end_latex

The update for $\mu_{lk}$ maximises the locus $l$, population $k$
terms in the above expression. Temporarily drop $l$ and $k$
subscripts, and let $p_i(k) = p_{\theta^*}(k|x_{li})$. Differentiating
the locus $l$, population $k$ terms in the above expression with
respect to $\mu$ and setting equal to zero gives

#+begin_latex 
\begin{align*}
\frac{\alpha F' -1}{\mu} - \frac{(1-\alpha) F' -1}{1-\mu} + \sum_i \( \frac{x_i}{\mu} - \frac{1-x_i}{1-\mu} \) p_i(k) = 0\\
\frac{1}{\mu(1-\mu)}\Bigg[(1-\mu)(\alpha F' -1) - \mu\((1-\alpha) F' -1\) + \sum_i \( (1-\mu)x_i - \mu(1-x_i) \) p_i(k)\Bigg] = 0\\
\alpha F' -1 - \mu\Bigg((1-\alpha) F' -1 + \alpha F' - 1 + \sum_i p_i(k)\Bigg) + \sum_i x_i p_i(k) = 0,\\
\end{align*}
#+end_latex
giving
\[
\mu = \frac{\sum_i x_i p_i(k) + \alpha F' -1}{\sum_i p_i(k) + F' - 2}
\]

* References
\bibliographystyle{genetres}
\bibliography{dan}
* Notes								   :noexport:
- Focus on SNP data?
** Alternative titles
Inference of population structure from large genotype data sets
Variational Bayes and parallel computing for fitting mixture models to genotype data
Inference of population structure from large genotype data sets: variational Bayes and parallel computing
* Config 							   :noexport:
** Org config
#+startup: oddeven content
#+begin_src emacs-lisp :exports results
(setq org-src-preserve-indentation t)
(setq org-latex-to-pdf-process '("make pdf"))

(setq org-export-latex-default-packages-alist
      (remove-if (lambda (el) (and (listp el) (equal (second el) "hyperref")))
		 org-export-latex-default-packages-alist))

(setq org-export-latex-classes
      '(("article"
	 "\\documentclass{article}
[PACKAGES]
[EXTRA]"
	 ("\\section{%s}" . "\\section*{%s}")
	 ("\\subsection{%s}" . "\\subsection*{%s}")
	 ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
	 ("\\paragraph{%s}" . "\\paragraph*{%s}")
	 ("\\subparagraph{%s}" . "\\subparagraph*{%s}"))))

(setq org-entities-user
      (mapcar (lambda (ent) (list ent ent t))
	      '("i" "j" "k" "K" "l" "n" "p" "P" "q" "Q" "X" "Z")))

(setq org-export-latex-packages-alist
      '(("" "newalg" t)))
#+end_src

#+results:
| i | i | t |
| j | j | t |
| k | k | t |
| K | K | t |
| l | l | t |
| n | n | t |
| p | p | t |
| P | P | t |
| q | q | t |
| Q | Q | t |
| X | X | t |
| Z | Z | t |
: t
** Makefile
#+begin_src makefile :tangle Makefile :noweb no
BASE = emvbpl
AUX = $(BASE).aux
BBL = $(BASE).bbl
TEX = $(BASE).tex
JUNK = $(BASE).toc $(BASE).out $(BASE).log $(BASE).blg $(BASE).dvi

LATEX = pdflatex
BIBTEX = bibtex


$(AUX): 
$(LATEX) $(BASE) > /dev/null

$(BIB): $(AUX)
bibtex $(BASE)

pdf: 	$(BIB)
make clean
$(LATEX) $(BASE) > /dev/null
bibtex $(BASE)
$(LATEX) $(BASE) > /dev/null
$(LATEX) $(BASE) > /dev/null

clean:
rm -f $(AUX) $(BBL) $(JUNK)
#+end_src

** emvbpl.sty
#+begin_src latex :tangle emvbpl.sty
\usepackage[sectionbib]{natbib}
\bibpunct{(}{)}{,}{a}{}{,}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{newalg}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage[left=2cm,top=3cm,right=3cm,head=2cm,foot=2cm]{geometry}
\newcommand{\E}{\text{E}{}}
\newcommand{\NL}{\nonumber\\}
\let\(\undefined
\let\)\undefined
\newcommand{\(}{\left(}
\newcommand{\)}{\right)}
\let\|\undefined
\newcommand{\|}{\arrowvert}
\renewcommand{\digamma}{\Psi}
\renewcommand*{\labelitemi}{\textbullet}
\renewcommand*{\labelitemii}{\labelitemi}
\renewcommand*{\labelitemiii}{\labelitemi}
\renewcommand*{\labelitemiv}{\labelitemi}

\usepackage{float}
\newfloat{algorithm}{thp}{lop}
\floatname{algorithm}{Algorithm}
#+end_src

** LaTeX headers
#+latex_header: \usepackage{emvbpl}
